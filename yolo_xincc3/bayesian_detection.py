import torch
import torch.nn as nn
import numpy as np
import cv2
import os
import shutil
from pathlib import Path
import matplotlib.pyplot as plt
from matplotlib.patches import Polygon
from typing import List, Dict, Tuple
import warnings
warnings.filterwarnings('ignore')
from ultralytics import YOLO
import os
os.environ["MKL_THREADING_LAYER"] = "GNU"
from tqdm import tqdm
from enum import Enum


# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šå‡ ä½•è®¡ç®—å·¥å…·å‡½æ•°
# ============================================================================

def ensure_ccw(vertices: np.ndarray) -> np.ndarray:
    """ç¡®ä¿å¤šè¾¹å½¢é¡¶ç‚¹æŒ‰é€†æ—¶é’ˆé¡ºåºæ’åˆ—"""
    n = len(vertices)
    if n < 3:
        return vertices
    
    signed_area = 0.0
    for i in range(n):
        j = (i + 1) % n
        signed_area += vertices[i, 0] * vertices[j, 1]
        signed_area -= vertices[j, 0] * vertices[i, 1]
    
    if signed_area < 0:
        return vertices[::-1].copy()
    return vertices


def polygon_area(vertices: np.ndarray) -> float:
    """è®¡ç®—å¤šè¾¹å½¢é¢ç§¯ï¼ˆShoelaceå…¬å¼ï¼‰"""
    n = len(vertices)
    if n < 3:
        return 0.0
    
    area = 0.0
    for i in range(n):
        j = (i + 1) % n
        area += vertices[i, 0] * vertices[j, 1]
        area -= vertices[j, 0] * vertices[i, 1]
    
    return abs(area) / 2.0


def polygon_intersection(poly1: np.ndarray, poly2: np.ndarray) -> np.ndarray:
    """è®¡ç®—ä¸¤ä¸ªå‡¸å¤šè¾¹å½¢çš„äº¤é›†ï¼ˆSutherland-Hodgmanç®—æ³•ï¼‰"""
    def inside_edge(point, edge_start, edge_end):
        cross = ((edge_end[0] - edge_start[0]) * (point[1] - edge_start[1]) -
                 (edge_end[1] - edge_start[1]) * (point[0] - edge_start[0]))
        return cross >= 0
    
    def compute_intersection(s, e, edge_start, edge_end):
        d1 = e - s
        d2 = edge_end - edge_start
        cross = d1[0] * d2[1] - d1[1] * d2[0]
        if abs(cross) < 1e-10:
            return s
        t = ((edge_start[0] - s[0]) * d2[1] - (edge_start[1] - s[1]) * d2[0]) / cross
        return s + t * d1
    
    output = poly1.copy()
    
    for i in range(len(poly2)):
        if len(output) == 0:
            break
        
        input_poly = output
        output = []
        
        edge_start = poly2[i]
        edge_end = poly2[(i + 1) % len(poly2)]
        
        for j in range(len(input_poly)):
            current = input_poly[j]
            next_vertex = input_poly[(j + 1) % len(input_poly)]
            
            curr_in = inside_edge(current, edge_start, edge_end)
            next_in = inside_edge(next_vertex, edge_start, edge_end)
            
            if curr_in:
                if next_in:
                    output.append(next_vertex)
                else:
                    output.append(compute_intersection(current, next_vertex, edge_start, edge_end))
            elif next_in:
                output.append(compute_intersection(current, next_vertex, edge_start, edge_end))
                output.append(next_vertex)
        
        output = np.array(output) if output else np.array([])
    
    return np.array(output) if len(output) > 0 else np.array([])


def calculate_obb_iou(obb1: np.ndarray, obb2: np.ndarray) -> float:
    """è®¡ç®—ä¸¤ä¸ªOBBçš„IoU"""
    obb1 = np.array(obb1).reshape(4, 2)
    obb2 = np.array(obb2).reshape(4, 2)
    
    obb1 = ensure_ccw(obb1)
    obb2 = ensure_ccw(obb2)
    
    area1 = polygon_area(obb1)
    area2 = polygon_area(obb2)
    
    if area1 < 1e-6 or area2 < 1e-6:
        return 0.0
    
    intersection = polygon_intersection(obb1, obb2)
    
    if len(intersection) < 3:
        return 0.0
    
    inter_area = polygon_area(intersection)
    union_area = area1 + area2 - inter_area
    
    return inter_area / union_area if union_area > 1e-6 else 0.0


def obb_to_xyxy(obb: np.ndarray) -> np.ndarray:
    """å°†OBBå››è§’ç‚¹è½¬æ¢ä¸ºè½´å¯¹é½è¾¹ç•Œæ¡†"""
    obb = np.array(obb).reshape(-1, 2)
    return np.array([np.min(obb[:, 0]), np.min(obb[:, 1]),
                     np.max(obb[:, 0]), np.max(obb[:, 1])])





# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šDropoutæ³¨å…¥å™¨
# ============================================================================
class DropoutInjector:
    def __init__(self, model, dropout_rate: float = 0.15):
        """
        Args:
            model: YOLOæ¨¡å‹
            dropout_rate: dropoutæ¦‚ç‡
        """
        self.model = model
        self.dropout_rate = dropout_rate
        self.hooks = []
        self.dropout_layers = []
        self.enabled = False
        
        # æŒ‡å®šè¦æ·»åŠ dropoutçš„å±‚
        self.target_layers = {
            # Backboneæ·±å±‚ç‰¹å¾
            'model.model.6',   # P4çš„C3k2è¾“å‡º  
            'model.model.8',   # P5çš„C3k2è¾“å‡º
            'model.model.9',   # SPPFè¾“å‡º
            'model.model.10',  # C2PSAè¾“å‡º
            
            # Headç‰¹å¾èåˆå±‚
            'model.model.13',  # P4 upsampleèåˆå
            'model.model.16',  # P3 upsampleèåˆå
            'model.model.19',  # P4 downsampleèåˆå
            'model.model.22',  # P5 downsampleèåˆå
        }
        
        self.device = next(model.model.parameters()).device
        self._register_hooks()
        if not self.hooks:
            raise RuntimeError(
                "No dropout hooks registered. Check target_layers for the current model."
            )

    
    def _register_hooks(self):
        """åªåœ¨æŒ‡å®šå±‚æ³¨å†Œhooks"""
        inner_model = self.model.model if hasattr(self.model, 'model') else self.model
        self._register_recursive(inner_model, 'model')
    
    def _register_recursive(self, module: nn.Module, prefix: str):
        """é€’å½’éå†ï¼Œç²¾ç¡®åŒ¹é…ç›®æ ‡å±‚"""
        for name, child in module.named_children():
            full_name = f"{prefix}.{name}" if prefix else name
            
            # ç²¾ç¡®åŒ¹é…ï¼šåªåœ¨å®Œæ•´è·¯å¾„åŒ¹é…æ—¶æ·»åŠ 
            if full_name in self.target_layers:
                self._add_dropout_hook(child)
            
            self._register_recursive(child, full_name)
    
    def _add_dropout_hook(self, module: nn.Module):
        """ä¸ºæŒ‡å®šæ¨¡å—æ·»åŠ dropout hook"""
        dropout = nn.Dropout2d(p=self.dropout_rate).to(self.device)
        dropout.train()
        idx = len(self.dropout_layers)
        self.dropout_layers.append(dropout)
        
        def make_hook(dropout_idx):
            def hook_fn(module, input, output):
                if self.enabled:
                    self.dropout_layers[dropout_idx].train()
                    return self.dropout_layers[dropout_idx](output)
                return output
            return hook_fn
        
        handle = module.register_forward_hook(make_hook(idx))
        self.hooks.append(handle)
    
    def enable(self):
        """å¯ç”¨dropout"""
        self.enabled = True
        for d in self.dropout_layers:
            d.train()
    
    def disable(self):
        """ç¦ç”¨dropout"""
        self.enabled = False
        for d in self.dropout_layers:
            d.eval()
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰hooks"""
        for handle in self.hooks:
            handle.remove()
        self.hooks.clear()
        self.dropout_layers.clear()


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šè´å¶æ–¯YOLOä¸»ç±»
# ============================================================================

class BayesianYOLO:
    """è´å¶æ–¯YOLOæ¨ç†å™¨ - OBBä¸“ç”¨ç‰ˆæœ¬"""
    
    def __init__(self, 
                 model_path: str,
                 device: str = 'cuda',
                 n_samples: int = 30,
                 dropout_rate: float = 0.1,
                 iou_threshold: float = 0.3,
                 min_samples: int = 5):
        
        self.device = device
        self.n_samples = n_samples
        self.iou_threshold = iou_threshold
        self.min_samples = min_samples
        self.model = YOLO(model_path, task='obb')
        self.model.to(device)
        
        self.dropout_injector = DropoutInjector(self.model, dropout_rate)
        
    


    def predict_with_uncertainty(
        self,
        source,
        conf: float = 0.25,
        iou: float = 0.45,
        imgsz: int = 640,
        tta_sources=None,
        **kwargs
    ) -> Tuple[List[Dict], Dict]:
        if tta_sources is None:
            if isinstance(source, (list, tuple)):
                sources = list(source)
            else:
                sources = [source]
        else:
            sources = list(tta_sources)

        total_samples = self.n_samples * len(sources)
        all_detections = []
        self.model.model.eval()
        self.dropout_injector.enable()

        # Debug: track confidence variation
        first_det_confs = []

        for idx in range(total_samples):
            src = sources[idx // self.n_samples]
            try:
                results = self.model.predict(
                    source=src, conf=conf, iou=iou,
                    imgsz=imgsz, verbose=False, **kwargs
                )

                if results and len(results) > 0:
                    result = results[0]
                    if hasattr(result, 'obb') and result.obb is not None and len(result.obb) > 0:
                        dets = self._extract_detections(result)
                        all_detections.append(dets)
                        if dets:
                            first_det_confs.append(dets[0]['conf'])
                    else:
                        all_detections.append([])
                else:
                    all_detections.append([])
            except Exception:
                all_detections.append([])

        self.dropout_injector.disable()

        return self._aggregate_detections(all_detections)

    def _extract_detections(self, result) -> List[Dict]:
        dets = []
        obb = result.obb
        corners = obb.xyxyxyxy.cpu().numpy()
        confs = obb.conf.cpu().numpy()
        classes = obb.cls.cpu().numpy()
        
        for i in range(len(corners)):
            dets.append({
                'obb': corners[i],
                'conf': float(confs[i]),
                'cls': int(classes[i])
            })
        return dets
    
    def _aggregate_detections(self, all_detections: List[List[Dict]]) -> Tuple[List[Dict], Dict]:
        # å±•å¹³
        all_dets = []
        for sample_idx, dets in enumerate(all_detections):
            for det in dets:
                det['sample_idx'] = sample_idx
                all_dets.append(det)
        
        if not all_dets:
            return [], {
                'avg_conf_std': 0,
                'num_detections_std': 0,
                'detection_stability': 1,
                'raw_detections': 0,
                'cluster_count': 0,
                'final_detections': 0,
                'clusters': []
            }
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        all_dets.sort(key=lambda x: x['conf'], reverse=True)
        
        # èšç±»
        clusters = []
        used = [False] * len(all_dets)
        
        for i, det in enumerate(all_dets):
            if used[i]:
                continue
            
            cluster = {
                'obbs': [det['obb']],
                'confs': [det['conf']],
                'cls': det['cls'],
                'samples': {det['sample_idx']}
            }
            used[i] = True
            
            for j in range(i + 1, len(all_dets)):
                if used[j] or all_dets[j]['cls'] != det['cls']:
                    continue
                
                iou = calculate_obb_iou(det['obb'], all_dets[j]['obb'])
                if iou >= self.iou_threshold:
                    cluster['obbs'].append(all_dets[j]['obb'])
                    cluster['confs'].append(all_dets[j]['conf'])
                    cluster['samples'].add(all_dets[j]['sample_idx'])
                    used[j] = True
            
            clusters.append(cluster)
        
        # ç»Ÿè®¡
        final = []
        cluster_summaries = []
        conf_stds = []
        
        for cluster_id, c in enumerate(clusters, start=1):
            n = len(c['samples'])
            obbs = np.array(c['obbs'])
            confs = np.array(c['confs'])
            
            summary = {
                'cluster_id': cluster_id,
                'obb': np.mean(obbs, axis=0),
                'conf': np.mean(confs),
                'conf_std': np.std(confs),
                'obb_std': np.mean(np.std(obbs, axis=0)),
                'cls': c['cls'],
                'num_samples': n
            }
            cluster_summaries.append(summary)
            
            if n < self.min_samples:
                continue
            final.append(summary.copy())
            conf_stds.append(np.std(confs))
        
        final.sort(key=lambda x: x['conf'], reverse=True)
        
        det_counts = [len(d) for d in all_detections]
        uncertainties = {
            'avg_conf_std': np.mean(conf_stds) if conf_stds else 0.0,
            'num_detections_std': np.std(det_counts),
            'detection_stability': 1.0 / (1.0 + np.std(det_counts)),
            'raw_detections': len(all_dets),
            'cluster_count': len(clusters),
            'final_detections': len(final),
            'clusters': cluster_summaries
        }
        
        return final, uncertainties
    
    def __del__(self):
        if hasattr(self, 'dropout_injector'):
            self.dropout_injector.remove_hooks()




# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šä¸ç¡®å®šæ€§åˆ†æå™¨
# ============================================================================

class ConfidenceLevel(Enum):
    """ç½®ä¿¡åº¦ç­‰çº§"""
    HIGH = "é«˜ç½®ä¿¡åº¦"
    MEDIUM = "ä¸­ç½®ä¿¡åº¦"
    LOW = "ä½ç½®ä¿¡åº¦"
    VERY_LOW = "æä½ç½®ä¿¡åº¦"


class UncertaintyAnalyzer:
    """
    å¤šç»´åº¦ä¸ç¡®å®šæ€§åˆ†æå™¨
    
    è¯„ä¼°ç»´åº¦ï¼š
    1. ç»å¯¹ç½®ä¿¡åº¦ (conf)
    2. ç›¸å¯¹ä¸ç¡®å®šæ€§ (conf_std / conf)  
    3. æ£€æµ‹ç¨³å®šæ€§ (num_samples / total_samples)
    4. ä½ç½®ä¸ç¡®å®šæ€§ (obb_std)
    """
    
    def __init__(self, 
                 total_samples: int = 30,
                 conf_threshold_high: float = 0.75,
                 conf_threshold_medium: float = 0.55,
                 relative_std_threshold_high: float = 0.05,
                 relative_std_threshold_low: float = 0.15,
                 stability_threshold: float = 0.60):
        """
        Args:
            total_samples: MCé‡‡æ ·æ€»æ¬¡æ•°
            conf_threshold_high: é«˜ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆç»å¯¹å€¼ï¼‰
            conf_threshold_medium: ä¸­ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆç»å¯¹å€¼ï¼‰
            relative_std_threshold_high: ç›¸å¯¹ä¸ç¡®å®šæ€§é«˜é˜ˆå€¼
            relative_std_threshold_low: ç›¸å¯¹ä¸ç¡®å®šæ€§ä½é˜ˆå€¼
            stability_threshold: ç¨³å®šæ€§é˜ˆå€¼ï¼ˆå‡ºç°ç‡ï¼‰
        """
        self.total_samples = total_samples
        self.conf_high = conf_threshold_high
        self.conf_medium = conf_threshold_medium
        self.rel_std_high = relative_std_threshold_high
        self.rel_std_low = relative_std_threshold_low
        self.stability_threshold = stability_threshold
    
    def calculate_quality_score(self, det: Dict) -> Tuple[float, Dict]:
        """
        è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•° (0-100)
        
        Returns:
            quality_score: ç»¼åˆåˆ†æ•°
            components: å„ç»´åº¦å¾—åˆ†è¯¦æƒ…
        """
        # 1. ç»å¯¹ç½®ä¿¡åº¦å¾—åˆ† (0-35, æŒ‡æ•°é¥±å’Œ)
        conf_score = 35 * (1 - np.exp(-det['conf'] / 0.25))
        
        # 2. ç›¸å¯¹ä¸ç¡®å®šæ€§å¾—åˆ† (0-20, è¶Šç¨³å®šè¶Šé«˜)
        relative_std = det['conf_std'] / max(det['conf'], 0.1)
        uncertainty_score = 20 * np.exp(-relative_std / 0.15)
        
        # 3. æ£€æµ‹ç¨³å®šæ€§å¾—åˆ† (0-35, æŒ‡æ•°é¥±å’Œ)
        stability = det['num_samples'] / self.total_samples
        stability_score = 35 * (1 - np.exp(-stability / 0.25))
        # è½»åº¦æƒ©ç½šï¼šä½å‡ºç°ç‡æ—¶ç•¥å¾®å‰Šå¼±ç½®ä¿¡åº¦åˆ†
        stability_factor = 0.8 + 0.2 * min(1.0, stability / 0.5)
        conf_score *= stability_factor
        
        # 4. ä½ç½®ç²¾åº¦å¾—åˆ† (0-10)
        obb_std = det.get('obb_std', 0)
        position_score = 10 * np.exp(-obb_std / 3.0)
        
        # ç»¼åˆåˆ†æ•°
        total_score = conf_score + uncertainty_score + stability_score + position_score
        
        components = {
            'conf_score': conf_score,
            'uncertainty_score': uncertainty_score,
            'stability_score': stability_score,
            'position_score': position_score,
            'relative_std': relative_std,
            'stability': stability,
            'stability_factor': stability_factor
        }
        
        return total_score, components
    
    def classify_detection(self, det: Dict) -> Tuple[ConfidenceLevel, str]:
        """
        ç»¼åˆåˆ†ç±»æ£€æµ‹è´¨é‡
        
        Returns:
            level: ç½®ä¿¡åº¦ç­‰çº§
            reason: åˆ†ç±»åŸå› 
        """
        quality_score, components = self.calculate_quality_score(det)
        
        conf = det['conf']
        relative_std = components['relative_std']
        stability = components['stability']
        
        # è§„åˆ™1: æä½ç½®ä¿¡åº¦ - æ— è®ºå…¶ä»–æŒ‡æ ‡å¦‚ä½•
        if conf < 0.50:
            return ConfidenceLevel.VERY_LOW, f"ç»å¯¹ç½®ä¿¡åº¦è¿‡ä½({conf:.3f})"
        
        # è§„åˆ™2: ä¸ç¨³å®šæ£€æµ‹ - å‡ºç°ç‡ä½
        if stability < self.stability_threshold:
            return ConfidenceLevel.LOW, f"æ£€æµ‹ä¸ç¨³å®š(å‡ºç°ç‡{stability:.1%})"
        
        # è§„åˆ™3: é«˜ç›¸å¯¹ä¸ç¡®å®šæ€§
        if relative_std > self.rel_std_low:
            if conf > self.conf_high:
                return ConfidenceLevel.MEDIUM, f"ç½®ä¿¡åº¦é«˜ä½†æ³¢åŠ¨å¤§(ç›¸å¯¹std={relative_std:.3f})"
            else:
                return ConfidenceLevel.LOW, f"ç›¸å¯¹ä¸ç¡®å®šæ€§è¿‡é«˜({relative_std:.3f})"
        
        # è§„åˆ™4: ç»¼åˆè¯„åˆ†
        if quality_score >= 80:
            return ConfidenceLevel.HIGH, f"ç»¼åˆè´¨é‡ä¼˜ç§€(åˆ†æ•°={quality_score:.1f})"
        elif quality_score >= 65:
            return ConfidenceLevel.MEDIUM, f"ç»¼åˆè´¨é‡è‰¯å¥½(åˆ†æ•°={quality_score:.1f})"
        elif quality_score >= 50:
            return ConfidenceLevel.LOW, f"ç»¼åˆè´¨é‡ä¸€èˆ¬(åˆ†æ•°={quality_score:.1f})"
        else:
            return ConfidenceLevel.VERY_LOW, f"ç»¼åˆè´¨é‡è¾ƒå·®(åˆ†æ•°={quality_score:.1f})"
    
    def analyze(self, detections: List[Dict], uncertainties: Dict) -> Dict:
        """
        ä¸»åˆ†ææ–¹æ³•
        """
        analysis = {
            'high_confidence': [],
            'medium_confidence': [],
            'low_confidence': [],
            'very_low_confidence': [],
            'statistics': {},
            'recommendations': [],
            'detailed_scores': []
        }
        
        for idx, det in enumerate(detections):
            # è®¡ç®—è´¨é‡åˆ†æ•°å’Œåˆ†ç±»
            quality_score, components = self.calculate_quality_score(det)
            level, reason = self.classify_detection(det)
            
            # æ„å»ºè¯¦ç»†ä¿¡æ¯
            info = {
                'idx': idx,
                'cls': det['cls'],
                'conf': det['conf'],
                'conf_std': det.get('conf_std', 0),
                'num_samples': det['num_samples'],
                'quality_score': quality_score,
                'level': level.value,
                'reason': reason,
                'components': components
            }
            
            # åˆ†ç±»
            if level == ConfidenceLevel.HIGH:
                analysis['high_confidence'].append(info)
            elif level == ConfidenceLevel.MEDIUM:
                analysis['medium_confidence'].append(info)
            elif level == ConfidenceLevel.LOW:
                analysis['low_confidence'].append(info)
            else:
                analysis['very_low_confidence'].append(info)
            
            analysis['detailed_scores'].append(info)
        
        # è®¡ç®—ç»Ÿè®¡
        total = len(detections)
        analysis['statistics'] = {
            'total_detections': total,
            'high_confidence_count': len(analysis['high_confidence']),
            'medium_confidence_count': len(analysis['medium_confidence']),
            'low_confidence_count': len(analysis['low_confidence']),
            'very_low_confidence_count': len(analysis['very_low_confidence']),
            'high_confidence_ratio': len(analysis['high_confidence']) / max(1, total),
            'avg_quality_score': np.mean([d['quality_score'] for d in analysis['detailed_scores']]) if detections else 0,
            'avg_confidence_std': uncertainties.get('avg_conf_std', 0),
            'detection_stability': uncertainties.get('detection_stability', 1),
            'raw_detections': uncertainties.get('raw_detections'),
            'cluster_count': uncertainties.get('cluster_count'),
            'final_detections': uncertainties.get('final_detections', total)
        }
        
        # ç”Ÿæˆå»ºè®®
        self._generate_recommendations(analysis)
        
        return analysis
    
    def _generate_recommendations(self, analysis: Dict):
        """ç”Ÿæˆæ™ºèƒ½å»ºè®®"""
        stats = analysis['statistics']
        recommendations = []
        
        if stats['total_detections'] == 0:
            recommendations.append("âš ï¸ æœªæ£€æµ‹åˆ°ç›®æ ‡")
            recommendations.append("å»ºè®®: é™ä½confé˜ˆå€¼æˆ–æ£€æŸ¥å›¾åƒè´¨é‡")
        else:
            # è´¨é‡åˆ†æ
            high_ratio = stats['high_confidence_ratio']
            avg_score = stats['avg_quality_score']
            
            if high_ratio >= 0.8 and avg_score >= 75:
                recommendations.append("âœ… æ•´ä½“è´¨é‡ä¼˜ç§€ï¼Œå¯è‡ªåŠ¨åŒ–å¤„ç†")
            elif high_ratio >= 0.6 and avg_score >= 65:
                recommendations.append("âœ“ æ•´ä½“è´¨é‡è‰¯å¥½ï¼Œå»ºè®®æŠ½æ£€20%")
            elif high_ratio >= 0.4:
                recommendations.append("âš ï¸ è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®äººå·¥å¤æ ¸50%")
            else:
                recommendations.append("ğŸ”´ è´¨é‡è¾ƒå·®ï¼Œéœ€è¦å…¨é¢äººå·¥å¤æ ¸")
            
            # å…·ä½“é—®é¢˜
            if stats['very_low_confidence_count'] > 0:
                recommendations.append(f"ğŸ”´ {stats['very_low_confidence_count']}ä¸ªæä½è´¨é‡æ£€æµ‹ï¼Œåº”åˆ é™¤")
            
            if stats['low_confidence_count'] > 0:
                recommendations.append(f"âš ï¸ {stats['low_confidence_count']}ä¸ªä½è´¨é‡æ£€æµ‹ï¼Œéœ€äººå·¥ç¡®è®¤")
            
            # ä¸ç¨³å®šæ€§è­¦å‘Š
            low_stability = [d for d in analysis['detailed_scores'] 
                           if d['components']['stability'] < 0.5]
            if low_stability:
                recommendations.append(f"âš ï¸ {len(low_stability)}ä¸ªæ£€æµ‹å‡ºç°ç‡<50%ï¼Œå¯èƒ½æ˜¯å‡é˜³æ€§")
            
            # é«˜æ³¢åŠ¨è­¦å‘Š
            high_variation = [d for d in analysis['detailed_scores']
                            if d['components']['relative_std'] > 0.15]
            if high_variation:
                recommendations.append(f"âš ï¸ {len(high_variation)}ä¸ªæ£€æµ‹æ³¢åŠ¨è¿‡å¤§ï¼Œå»ºè®®å¤æ ¸")
        
        analysis['recommendations'] = recommendations
    

    def build_report(self, analysis: Dict) -> str:
        """ç»„è£…è¯¦ç»†æŠ¥å‘Šæ–‡æœ¬"""
        lines = []
        lines.append("=" * 80)
        lines.append("å¤šç»´åº¦ä¸ç¡®å®šæ€§åˆ†ææŠ¥å‘Š")
        lines.append("=" * 80)
        lines.append("")

        stats = analysis['statistics']
        lines.append("ã€æ€»ä½“ç»Ÿè®¡ã€‘")
        raw_detections = stats.get('raw_detections')
        cluster_count = stats.get('cluster_count')
        final_detections = stats.get('final_detections', stats['total_detections'])
        if raw_detections is not None:
            lines.append(f"  æ€»æ£€æµ‹æ•°: {raw_detections}")
        if cluster_count is not None:
            lines.append(f"  èšç±»æ•°: {cluster_count}")
        lines.append(f"  æœ€ç»ˆæ£€æµ‹æ•°: {final_detections}")
        lines.append(f"  å¹³å‡è´¨é‡åˆ†æ•°: {stats['avg_quality_score']:.1f}/100")
        lines.append(f"  å¹³å‡conf_std: {stats['avg_confidence_std']:.4f}")
        lines.append(f"  æ£€æµ‹ç¨³å®šæ€§: {stats['detection_stability']:.4f}")
        lines.append("")

        lines.append("ã€è´¨é‡åˆ†å¸ƒã€‘")
        lines.append(f"  â­â­â­ é«˜ç½®ä¿¡åº¦: {stats['high_confidence_count']} ({stats['high_confidence_ratio']:.1%})")
        lines.append(f"  â­â­   ä¸­ç½®ä¿¡åº¦: {stats['medium_confidence_count']}")
        lines.append(f"  â­     ä½ç½®ä¿¡åº¦: {stats['low_confidence_count']}")
        lines.append(f"  âŒ     æä½ç½®ä¿¡åº¦: {stats['very_low_confidence_count']}")

        for level_name, level_key, emoji in [
            ("é«˜ç½®ä¿¡åº¦æ£€æµ‹", "high_confidence", "â­â­â­"),
            ("ä¸­ç½®ä¿¡åº¦æ£€æµ‹", "medium_confidence", "â­â­"),
            ("ä½ç½®ä¿¡åº¦æ£€æµ‹", "low_confidence", "â­"),
            ("æä½ç½®ä¿¡åº¦æ£€æµ‹", "very_low_confidence", "âŒ"),
        ]:
            dets = analysis[level_key]
            if dets:
                lines.append("")
                lines.append(f"ã€{level_name}ã€‘{emoji}")
                for d in dets:
                    lines.append(
                        f"  #{d['idx']}: conf={d['conf']:.3f}, "
                        f"std={d['conf_std']:.4f}, "
                        f"å‡ºç°ç‡={d['components']['stability']:.1%}, "
                        f"è´¨é‡={d['quality_score']:.1f}"
                    )
                    lines.append(f"         åŸå› : {d['reason']}")

        lines.append("=" * 80)
        return "\n".join(lines)

    def export_review_list(self, analysis: Dict, output_file: str = None):
        """å¯¼å‡ºéœ€è¦äººå·¥å¤æ ¸çš„æ£€æµ‹åˆ—è¡¨"""
        review_list = []
        
        # ä¼˜å…ˆçº§1: æä½è´¨é‡
        for d in analysis['very_low_confidence']:
            review_list.append({
                'idx': d['idx'],
                'priority': 'HIGH',
                'conf': d['conf'],
                'quality_score': d['quality_score'],
                'reason': d['reason'],
                'action': 'DELETE'
            })
        
        # ä¼˜å…ˆçº§2: ä½è´¨é‡
        for d in analysis['low_confidence']:
            review_list.append({
                'idx': d['idx'],
                'priority': 'MEDIUM',
                'conf': d['conf'],
                'quality_score': d['quality_score'],
                'reason': d['reason'],
                'action': 'REVIEW'
            })
        
        # ä¼˜å…ˆçº§3: ä¸­ç­‰è´¨é‡ï¼ˆé€‰æ‹©æ€§ï¼‰
        for d in analysis['medium_confidence']:
            if d['quality_score'] < 70:
                review_list.append({
                    'idx': d['idx'],
                    'priority': 'LOW',
                    'conf': d['conf'],
                    'quality_score': d['quality_score'],
                    'reason': d['reason'],
                    'action': 'OPTIONAL_REVIEW'
                })
        
        # æŒ‰ä¼˜å…ˆçº§å’Œè´¨é‡åˆ†æ•°æ’åº
        priority_order = {'HIGH': 0, 'MEDIUM': 1, 'LOW': 2}
        review_list.sort(key=lambda x: (priority_order[x['priority']], x['quality_score']))
        
        if output_file:
            import json
            with open(output_file, 'w') as f:
                json.dump(review_list, f, indent=2)
        
        return review_list

# è®¾ç½® matplotlib çš„ä¸­æ–‡å­—ä½“æ”¯æŒ
def setup_font():
    import matplotlib.font_manager as fm
    fonts = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    available = [f.name for f in fm.fontManager.ttflist]
    for font in fonts:
        if font in available:
            plt.rcParams['font.sans-serif'] = [font]
            plt.rcParams['axes.unicode_minus'] = False
            return font
    return None


class BayesianYOLOInference:
    def __init__(self, model_path: str, device: str = 'cuda',
                 n_samples: int = 30, dropout_rate: float = 0.2,
                 conf_threshold: float = 0.25, iou_threshold: float = 0.45):
        
        self.n_samples = n_samples
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        
        setup_font()
        
        self.bayesian_model = BayesianYOLO(
            model_path=model_path,
            device=device,
            n_samples=n_samples,
            dropout_rate=dropout_rate,
            iou_threshold=iou_threshold,
            min_samples=7,
        )
        
        self.analyzer = UncertaintyAnalyzer(total_samples=n_samples)
    
    def inference(
        self,
        image_path: str,
        output_dir: str = './results',
        predict_dir: str = None
    ) -> Tuple[List[Dict], Dict, Dict]:

        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"æ— æ³•è¯»å–: {image_path}")
        
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        tta_images = self._generate_tta_images(image)
        total_samples = self.n_samples * len(tta_images)
        self.analyzer.total_samples = total_samples

        detections, uncertainties = self.bayesian_model.predict_with_uncertainty(
            source=image,
            conf=self.conf_threshold,
            iou=self.iou_threshold,
            imgsz=960,
            tta_sources=tta_images
        )
        
        analysis = self.analyzer.analyze(detections, uncertainties)
        analysis_report = self.analyzer.build_report(analysis)
        
        visual_dir = output_dir
        if predict_dir:
            visual_dir = os.path.join(predict_dir, 'report')
        self._save_results(image_rgb, detections, uncertainties,
                           visual_dir, os.path.basename(image_path),
                           analysis_report=analysis_report,
                           total_samples=total_samples)
        if predict_dir:
            self._save_predict_outputs(image_path, image, detections, analysis, predict_dir)
        
        return detections, uncertainties, analysis
    
    def _save_results(
        self,
        image,
        detections,
        uncertainties,
        output_dir,
        name,
        analysis_report=None,
        total_samples=None
    ):
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        self._save_report(detections, uncertainties, output_dir, name, analysis_report, total_samples)

    def _save_predict_outputs(self, image_path, image, detections, analysis, output_dir):
        output_path = Path(output_dir)
        labels_dir = output_path / 'labels'
        labels_visual_dir = output_path / 'visual'
        output_path.mkdir(parents=True, exist_ok=True)
        labels_dir.mkdir(parents=True, exist_ok=True)
        labels_visual_dir.mkdir(parents=True, exist_ok=True)

        image_name = Path(image_path).name
        target_image = output_path / image_name
        try:
            shutil.copy2(image_path, target_image)
        except Exception:
            cv2.imwrite(str(target_image), image)

        label_name = f"{Path(image_path).stem}.txt"
        label_path = labels_dir / label_name

        selected_indices = {
            info['idx']
            for info in (analysis.get('high_confidence', []) + analysis.get('medium_confidence', []))
        }

        with open(label_path, 'w', encoding='utf-8') as f:
            for idx, det in enumerate(detections):
                if idx not in selected_indices:
                    continue
                line = self._format_obb_label(det, image.shape)
                if line:
                    f.write(line + "\n")

        self._save_label_visual(image, label_path, labels_visual_dir, image_name)

    def _format_obb_label(self, det, image_shape):
        obb = np.array(det['obb']).reshape(-1, 2).astype(float)
        h, w = image_shape[:2]
        if w <= 0 or h <= 0:
            return ""

        obb[:, 0] /= w
        obb[:, 1] /= h
        coords = obb.reshape(-1)
        cls_id = int(det.get('cls', 0))
        conf = float(det.get('conf', 0.0))

        parts = [str(cls_id)]
        parts.extend([f"{v:.6f}" for v in coords])
        parts.append(f"{conf:.6f}")
        return " ".join(parts)

    def _class_color(self, cls_id: int) -> Tuple[int, int, int]:
        palette = [
            (0, 255, 0),
            (0, 0, 255),
            (255, 0, 0),
            (0, 255, 255),
            (255, 0, 255),
            (255, 255, 0),
        ]
        return palette[int(cls_id) % len(palette)]

    def _save_label_visual(self, image, label_path: Path, output_dir: Path, image_name: str):
        if not label_path.exists():
            return
        h, w = image.shape[:2]
        if w <= 0 or h <= 0:
            return

        visual = image.copy()
        lines = label_path.read_text(encoding='utf-8').splitlines()
        for line in lines:
            parts = line.strip().split()
            if len(parts) < 9:
                continue
            try:
                cls_id = int(float(parts[0]))
                values = [float(v) for v in parts[1:]]
            except ValueError:
                continue
            if len(values) < 8:
                continue

            coords = values[:8]
            conf = values[8] if len(values) >= 9 else None
            pts = np.array(coords, dtype=np.float32).reshape(-1, 2)
            pts[:, 0] *= w
            pts[:, 1] *= h

            color = self._class_color(cls_id)
            pts_int = np.round(pts).astype(int).reshape(-1, 1, 2)
            cv2.polylines(visual, [pts_int], True, color, 2, lineType=cv2.LINE_AA)

            label = f"{cls_id}"
            if conf is not None:
                label = f"{cls_id}:{conf:.2f}"
            x, y = pts_int[0, 0]
            y = max(0, y - 4)
            cv2.putText(
                visual,
                label,
                (x, y),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                color,
                1,
                cv2.LINE_AA,
            )

        output_path = output_dir / image_name
        cv2.imwrite(str(output_path), visual)

    def _generate_tta_images(self, image):
        # Conservative pixel-only TTA (no coordinate transforms).
        images = [image]
        images.append(self._brightness_add(image, 8))
        images.append(self._brightness_mul(image, 1.03))
        images.append(self._contrast(image, 1.03))
        images.append(self._gamma(image, 1.05))
        images.append(self._saturation(image, 1.03))
        images.append(self._sharpen(image, 0.2))
        images.append(self._gaussian_noise(image, 3.0))
        return images

    def _brightness_add(self, image, delta):
        out = image.astype(np.float32) + float(delta)
        return np.clip(out, 0, 255).astype(np.uint8)

    def _brightness_mul(self, image, factor):
        out = image.astype(np.float32) * float(factor)
        return np.clip(out, 0, 255).astype(np.uint8)

    def _contrast(self, image, factor):
        out = (image.astype(np.float32) - 128.0) * float(factor) + 128.0
        return np.clip(out, 0, 255).astype(np.uint8)

    def _gamma(self, image, gamma):
        inv_gamma = 1.0 / float(gamma)
        table = (np.arange(256) / 255.0) ** inv_gamma * 255.0
        table = np.clip(table, 0, 255).astype(np.uint8)
        return cv2.LUT(image, table)

    def _saturation(self, image, factor):
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV).astype(np.float32)
        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * float(factor), 0, 255)
        return cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)

    def _sharpen(self, image, amount):
        blur = cv2.GaussianBlur(image, (0, 0), sigmaX=1.0)
        return cv2.addWeighted(image, 1.0 + float(amount), blur, -float(amount), 0)

    def _gaussian_noise(self, image, sigma):
        rng = np.random.default_rng(0)
        noise = rng.normal(0, float(sigma), image.shape).astype(np.float32)
        out = image.astype(np.float32) + noise
        return np.clip(out, 0, 255).astype(np.uint8)
    
    def _visualize(self, image, detections, uncertainties, output_dir, name):
        fig, axes = plt.subplots(1, 2, figsize=(16, 8))
        
        ax1 = axes[0]
        ax1.imshow(image)
        ax1.set_title('Detection Results', fontsize=14, fontweight='bold')
        ax1.axis('off')
        
        cluster_summaries = None
        if uncertainties:
            cluster_summaries = uncertainties.get('clusters')

        if cluster_summaries:
            cmap = plt.cm.get_cmap('tab20', max(1, len(cluster_summaries)))
            for idx, cluster in enumerate(cluster_summaries):
                obb = np.array(cluster['obb']).reshape(-1, 2)
                color = cmap(idx)

                polygon = Polygon(obb, linewidth=2, edgecolor=color, facecolor='none')
                ax1.add_patch(polygon)

                score, _ = self.analyzer.calculate_quality_score(cluster)
                label_x, label_y = np.min(obb[:, 0]), np.min(obb[:, 1]) - 5
                cluster_id = cluster.get('cluster_id', idx + 1)
                label = f'ID:{cluster_id} N:{cluster["num_samples"]} Q:{score:.1f}'
                ax1.text(label_x, label_y, label, fontsize=8, color='white',
                        fontweight='bold', bbox=dict(boxstyle='round,pad=0.2',
                        facecolor=color, alpha=0.7))
        elif detections:
            for idx, det in enumerate(detections):
                obb = det['obb']
                conf = det['conf']
                conf_std = det.get('conf_std', 0)
                
                color = 'green' if conf_std < 0.02 else ('orange' if conf_std < 0.05 else 'red')
                
                polygon = Polygon(obb, linewidth=2, edgecolor=color, facecolor='none')
                ax1.add_patch(polygon)
                
                label_x, label_y = np.min(obb[:, 0]), np.min(obb[:, 1]) - 5
                label = f'#{idx+1} C:{conf:.2f} S:{conf_std:.3f}'
                ax1.text(label_x, label_y, label, fontsize=8, color='white',
                        fontweight='bold', bbox=dict(boxstyle='round,pad=0.2', 
                        facecolor=color, alpha=0.8))
        else:
            ax1.text(0.5, 0.5, 'No Detection', transform=ax1.transAxes,
                    fontsize=16, ha='center', va='center',
                    bbox=dict(facecolor='yellow', alpha=0.7))
        
        ax2 = axes[1]
        ax2.imshow(image)
        ax2.set_title('Uncertainty Heatmap', fontsize=14, fontweight='bold')
        ax2.axis('off')
        
        if detections:
            heatmap = np.zeros(image.shape[:2], dtype=np.float32)
            for det in detections:
                xyxy = obb_to_xyxy(det['obb']).astype(int)
                x1, y1 = max(0, xyxy[0]), max(0, xyxy[1])
                x2, y2 = min(image.shape[1], xyxy[2]), min(image.shape[0], xyxy[3])
                if x2 > x1 and y2 > y1:
                    heatmap[y1:y2, x1:x2] = np.maximum(
                        heatmap[y1:y2, x1:x2], det.get('conf_std', 0))
            
            im = ax2.imshow(heatmap, alpha=0.6, cmap='jet', vmin=0, vmax=0.1)
            plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04, label='Uncertainty')
        
        plt.tight_layout()
        save_path = os.path.join(output_dir, f'visualization_{name}')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
    
    def _save_report(self, detections, uncertainties, output_dir, name,
                     analysis_report=None, total_samples=None):
        path = os.path.join(output_dir, f'report_{name}.txt')
        sample_total = total_samples if total_samples else self.n_samples
        
        with open(path, 'w', encoding='utf-8') as f:
            if analysis_report:
                f.write(analysis_report + "\n\n")
            f.write("="*60 + "\n")
            f.write(f"æ£€æµ‹æŠ¥å‘Š - {name}\n")
            f.write("="*60 + "\n\n")
            f.write(f"æ£€æµ‹æ€»æ•°: {len(detections)}\n")
            f.write(f"å¹³å‡conf_std: {uncertainties.get('avg_conf_std', 0):.4f}\n\n")

            cluster_summaries = uncertainties.get('clusters', [])
            f.write("èšç±»ä¿¡æ¯:\n")
            if cluster_summaries:
                for c in cluster_summaries:
                    score, _ = self.analyzer.calculate_quality_score(c)
                    f.write(
                        f"  ID:{c.get('cluster_id','-')} cls:{c.get('cls',0)} "
                        f"conf:{c.get('conf',0):.4f} std:{c.get('conf_std',0):.4f} "
                        f"samples:{c.get('num_samples',0)}/{sample_total} "
                        f"score:{score:.1f}\n"
                    )
            else:
                f.write("  æ— èšç±»ç»“æœ\n")
            f.write("\n")
            
            for idx, det in enumerate(detections):
                obb = det['obb']
                xyxy = obb_to_xyxy(obb)
                f.write(f"#{idx+1}:\n")
                f.write(f"  ç½®ä¿¡åº¦: {det['conf']:.4f} Â± {det.get('conf_std', 0):.4f}\n")
                f.write(f"  æ ·æœ¬æ•°: {det['num_samples']}/{sample_total}\n")
                f.write(f"  è¾¹ç•Œ: ({xyxy[0]:.0f},{xyxy[1]:.0f})-({xyxy[2]:.0f},{xyxy[3]:.0f})\n")
                f.write(f"  å››è§’ç‚¹: {obb.tolist()}\n\n")
        


def main():
    # ===================== é…ç½® =====================
    MODEL_PATH = './results/11s_base_3/train/weights/best.pt'
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    N_SAMPLES = 30
    DROPOUT_RATE = 0.15
    CONF_THRESHOLD = 0.25
    IOU_THRESHOLD = 0.45
    IMAGE_PATH = './data_crop/images'
    IMAGE_EXTS = {'.png', '.jpg', '.jpeg', '.bmp', '.gif'}
    OUTPUT_DIR = './results_bayesian'
    PREDICT_DIR = './results_bayesian/predict'
    # ================================================
    
    input_path = Path(IMAGE_PATH)
    if input_path.is_dir():
        image_paths = [
            p for p in sorted(input_path.iterdir())
            if p.is_file() and p.suffix.lower() in IMAGE_EXTS
        ]
        if not image_paths:
            return
    elif input_path.is_file():
        if input_path.suffix.lower() not in IMAGE_EXTS:
            return
        image_paths = [input_path]
    else:
        return

    try:
        # è´å¶æ–¯æ¨ç†
        engine = BayesianYOLOInference(
            model_path=MODEL_PATH, device=DEVICE,
            n_samples=N_SAMPLES, dropout_rate=DROPOUT_RATE,
            conf_threshold=CONF_THRESHOLD, iou_threshold=IOU_THRESHOLD
        )

        if input_path.is_dir():
            total_images = len(image_paths)
            for idx, image_path in enumerate(image_paths, start=1):
                print(f"[{idx}/{total_images}] {image_path.name}")
                engine.inference(
                    str(image_path),
                    OUTPUT_DIR,
                    predict_dir=PREDICT_DIR
                )
        else:
            for image_path in image_paths:
                engine.inference(
                    str(image_path),
                    OUTPUT_DIR,
                    predict_dir=PREDICT_DIR
                )
        
    except Exception:
        raise


def bayesian_detection(imaege_path, output_dir):
    
    current_dir = os.path.dirname(os.path.abspath(__file__))
    model_file = os.path.join(current_dir, 'model/follicle/best.pt') 
    MODEL_PATH = model_file

    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    N_SAMPLES = 25
    DROPOUT_RATE = 0.15
    CONF_THRESHOLD = 0.25
    IOU_THRESHOLD = 0.45
    IMAGE_EXTS = {'.png', '.jpg', '.jpeg', '.bmp', '.gif'}

    IMAGE_PATH = imaege_path
    OUTPUT_DIR = output_dir
    PREDICT_DIR = output_dir + '/predict'
    
    print(f"\né…ç½®: device={DEVICE}, n={N_SAMPLES}, dropout={DROPOUT_RATE}")
    
    input_path = Path(IMAGE_PATH)
    if input_path.is_dir():
        image_paths = [
            p for p in sorted(input_path.iterdir())
            if p.is_file() and p.suffix.lower() in IMAGE_EXTS
        ]
        if not image_paths:
            print(f"\nâŒ ç›®å½•ä¸­æ²¡æœ‰æ”¯æŒçš„å›¾ç‰‡: {IMAGE_PATH}")
            return
    elif input_path.is_file():
        if input_path.suffix.lower() not in IMAGE_EXTS:
            print(f"\nâŒ ä¸æ”¯æŒçš„å›¾ç‰‡æ ¼å¼: {IMAGE_PATH}")
            return
        image_paths = [input_path]
    else:
        print(f"\nâŒ å›¾åƒä¸å­˜åœ¨: {IMAGE_PATH}")
        return


    engine = BayesianYOLOInference(
        model_path=MODEL_PATH, device=DEVICE,
        n_samples=N_SAMPLES, dropout_rate=DROPOUT_RATE,
        conf_threshold=CONF_THRESHOLD, iou_threshold=IOU_THRESHOLD
    )
    
    for image_path in image_paths:
        detections, uncertainties, analysis = engine.inference(
            str(image_path),
            OUTPUT_DIR,
            predict_dir=PREDICT_DIR
        )
        if detections:
            total_samples = engine.analyzer.total_samples
            print(f"\nå„æ£€æµ‹: {image_path.name}")
            for i, d in enumerate(detections):
                print(f"  #{i+1}: conf={d['conf']:.3f}, std={d.get('conf_std',0):.4f}, "
                        f"samples={d['num_samples']}/{total_samples}")
        